{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset Construction'''\n",
    "# Parent directories, 'orginal' stores the original chapters translated by machine or by different translators; \n",
    "#                     'cleaned' stores the cleaned files by data pre-processing\n",
    "ORIGINAL_DIR = Path('original')\n",
    "CLEANED_DIR = Path('cleaned')\n",
    "\n",
    "# Chuildren directories for each parent dir\n",
    "PO_GOO_DIR = Path('po-goo') # stores chapters translated by Google Translate from the original version (Polish) of the book Solaris to English\n",
    "PO_DL_DIR = Path('po-dl') # stores chapters translated by DeepL API from the original version (Polish) of the book Solaris to English\n",
    "FR_GOO_DIR = Path('fr-goo') # stores chapters translated by Google Translate from the translated French version of the book Solaris to English\n",
    "FR_DL_DIR = Path('fr-dl') # stores chapters translated by DeepL API from the translated French version of the book Solaris to English\n",
    "EN1_DIR = Path('fr-h') # stores chapters of first translated version to English of the book Solaris\n",
    "EN2_DIR = Path('po-h') # stores chapters of second translated version to English of the book Solaris\n",
    "\n",
    "# List to access children directories\n",
    "DIRS = [PO_GOO_DIR, PO_DL_DIR, FR_GOO_DIR, FR_DL_DIR, EN1_DIR, EN2_DIR]\n",
    "\n",
    "# List for all the translations\n",
    "BOOKS = ['po-goo', 'po-dl', 'fr-goo', 'fr-dl', 'fr-h', 'po-h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data analysis'''\n",
    "# Find out the replacement by checking the vocabularies in parellel chapters for confirmation\n",
    "TO_REPLACE = {\n",
    "    'ö': 'o',\n",
    "    'é': 'e',\n",
    "    'ë': 'e',\n",
    "    'ï': 'i',\n",
    "    'ą': 'a',\n",
    "    'í': 'i',\n",
    "    'æ': 'ae',\n",
    "    'ð': 'ny', # Original - fr-dl[8] 'caðon', changed to based on po-en-goo 'Canyon'\n",
    "    'à': 'a',\n",
    "    'å': 'a',\n",
    "    'â': 'a',\n",
    "    \"'\": '', # Replace apostrophe with nothing to protect contractions like don't -> dont\n",
    "    '’': '',\n",
    "    ' `': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ą', '—', 'ð', '»', 'ï', ':', ';', 'ö', '?', '\"', 'å', '\\x0c', 'ë', '³', 'é', 'à', '_', '’', '^', '…', 'â', '(', ')', 'í', '‘', '•', '–', '.', '-', '*', '`', '!', \"'\", '”', '“', 'æ', ','}\n"
     ]
    }
   ],
   "source": [
    "# Function to check special characters by checking each translated texts.\n",
    "allowed=\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \\n\"\n",
    "def get_not_allowed(txt):\n",
    "    others = set()\n",
    "    for i in txt:\n",
    "        if i not in allowed:\n",
    "            others.add(i)\n",
    "    return others\n",
    "\n",
    "others = set()\n",
    "for p in DIRS:\n",
    "    for i in (ORIGINAL_DIR / p).iterdir(): # Checked by each directory contained target data\n",
    "        with open(i) as f:\n",
    "            txt = f.read()\n",
    "            # Change the html punctuation format to the normal one\n",
    "            txt = html.unescape(txt)\n",
    "        others = others.union(get_not_allowed(txt))\n",
    "print(others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is by manually selection based on the source\n",
    "PROPER_NOUNS = ['harey','snaut','sartorius','kelvin','gibarian','kris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning'''\n",
    "\n",
    "for i in DIRS:\n",
    "    for j in (ORIGINAL_DIR / i).iterdir():\n",
    "        with open(j) as f:\n",
    "            txt = f.read()\n",
    "        \n",
    "        # Change the html punctuation format to the normal one\n",
    "        txt = html.unescape(txt)\n",
    "\n",
    "        # Replace special characters\n",
    "        for key in TO_REPLACE.keys():\n",
    "            txt = txt.replace(key, TO_REPLACE[key])\n",
    "\n",
    "        # Relace the rest of the characters with empty string\n",
    "        for each in others:\n",
    "            txt = txt.replace(each, ' ')\n",
    "        \n",
    "        # Lowercase everything, incase it counts \"I\" and 'i' as different words\n",
    "        txt = txt.lower()\n",
    "\n",
    "        # Exclude proper nouns\n",
    "        for each in PROPER_NOUNS:\n",
    "            txt = txt.replace(each, ' ')\n",
    "        \n",
    "        if not (CLEANED_DIR / i).is_dir():\n",
    "            (CLEANED_DIR / i).mkdir(parents=True)\n",
    "\n",
    "        # Write to the cleaned directory\n",
    "        with open(CLEANED_DIR / i / j.name, 'w') as f:\n",
    "            f.write(txt)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
